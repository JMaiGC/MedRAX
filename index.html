<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="MedRAX, Chest X-ray AI, Medical AI, CXR interpretation, Medical reasoning, AI in radiology, ChestAgentBench, Medical VQA, AI healthcare, Automated diagnosis">
    <meta name="description" content="MedRAX is an advanced AI agent for Chest X-ray interpretation, seamlessly integrating multimodal reasoning and structured tool-based decision-making to improve complex medical query analysis. MedRAX outperforms existing models, setting a new benchmark in automated CXR diagnosis with ChestAgentBench.">    
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <!--Main Scripts-->
    <link rel="stylesheet" type="text/css" href="style.css">
    <script src="script.js" defer=""></script>
    <!-- MatJax -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript" async=""
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML">
        </script>
    <!--Jquery-->
    <script type="text/javascript" src="https://code.jquery.com/jquery-1.11.0.min.js" defer=""></script>
    <script type="text/javascript" src="https://code.jquery.com/jquery-migrate-1.2.1.min.js" defer=""></script>
    <title>MedRAX: Medical Reasoning Agent for Chest X-ray</title>
</head>

<body>
    <div class="toggle-container">
        <button id="toggleButton" class="inactive">Dark Mode</button>

        <div id="navigateAndButton">
            <a href=#menu class="scroll-link"><button id="jumpTo">Navigate</button></a>
            <div id="navigation-content">
                <div id="navigation">
                    <span class="inline-heading-2"><a href=#header class="scroll-link">Return to Top</a></span>
                    <span class="inline-heading"><a href=#Abstract class="scroll-link">1. Abstract</a></span>
                    <span class="inline-heading"><a href=#KeyContributions class="scroll-link">2. Key Contributions</a></span>
                    <span class="inline-heading"><a href=#MedRAXFramework class="scroll-link">3. MedRAX Framework</a></span>
                    <span class="inline-heading"><a href=#ChestAgentBench class="scroll-link">4. ChestAgentBench</a></span>
                    <span class="inline-heading"><a href=#Experiments class="scroll-link">5. Experiments</a></span>
                    <span class="inline-heading"><a href=#CaseStudy class="scroll-link">6. Case Study</a></span>
                    <p><a href="#6a" class="scroll-link">a. Medical Device Identification (Eurorad Case 17576)</a></p>
                    <p><a href="#6b" class="scroll-link">b. Multi-step Disease Diagnosis (Eurorad Case 16703)</a></p>
                    <span class="inline-heading"><a href=#Conclusion class="scroll-link">7. Conclusion</a></span>
                    <span class="inline-heading"><a href=#Reference class="scroll-link">8. Reference</a></span>
                    <span class="inline-heading"><a href=#BibTeX class="scroll-link">9. BibTeX</a></span>
                </div>
            </div>
        </div>
    </div>

    <div class="outer-container">

        <div class="header" , id="header">
            <h1>MedRAX</h1>
            <p class="subheading">MedRAX: Medical Reasoning Agent for Chest X-ray</p>
            <p class="paper-links">
                <div class="center-text">
                    <a href="https://arxiv.org/abs/2502.02673" target="_blank" rel="noopener">
                        <img src="https://img.shields.io/badge/arXiv-Paper-FF6B6B?style=for-the-badge&logo=arxiv&logoColor=white" alt="arXiv">
                    </a>
                    <a href="https://github.com/bowang-lab/MedRAX" target="_blank" rel="noopener">
                        <img src="https://img.shields.io/badge/GitHub-Code-4A90E2?style=for-the-badge&logo=github&logoColor=white" alt="GitHub">
                    </a>
                    <a href="https://huggingface.co/datasets/wanglab/chest-agent-bench" target="_blank" rel="noopener">
                        <img src="https://img.shields.io/badge/HuggingFace-Dataset-FFBF00?style=for-the-badge&logo=huggingface&logoColor=white" alt="HuggingFace Dataset">
                    </a>
                </div>
            </p>
            <p class="authors">
                <span class="author" id="author1">Adibvafa Fallahpour<sup>1,2,3</sup> (adibvafa.fallahpour@mail.utoronto.ca)</span>,
                <span class="author" id="author2">Jun Ma<sup>2,3</sup></span>,
                <span class="author" id="author3">Alif Munim<sup>3,4</sup></span>,
                <span class="author" id="author4">Hongwei Lyu<sup>3</sup></span>,
                <span class="author" id="author5">Bo Wang<sup>1,2,3,5</sup></span>,
            </p>
            <p class="associations">
                <span class="associations"><sup>1</sup>Department of Computer Science, University of Toronto, Toronto, Canada</span><br>
                <span class="associations"><sup>2</sup>Vector Institute, Toronto, Canada</span><br>
                <span class="associations"><sup>3</sup>University Health Network, Toronto, Canada</span><br>
                <span class="associations"><sup>4</sup>Cohere For AI, Toronto, Canada</span><br>
                <span class="associations"><sup>5</sup>Department of Laboratory Medicine and Pathobiology, University of Toronto, Toronto, Canada</span><br>
            </p>

            <!-- <p>The dataset, code, and trained model weights will be publicly available after anonymous peer review.</p> -->
        </div>

        <div class="video-grid">
            <div class="video-container">
                <video id='demoVideo' src="static/videos/video.mp4" controls autoplay loop></video>
            </div>
        </div>
        
        <div class="section" , id="Abstract">
            <h2>Abstract</h2>
            <div class="subsection">
                <p>Chest X-rays (CXRs) play an integral role in driving critical decisions in disease management and patient care. While recent innovations have led to specialized models for various CXR interpretation tasks, these solutions often operate in isolation, limiting their practical utility in clinical practice.</p>
                <p>We present MedRAX, the first versatile AI agent that seamlessly integrates state-of-the-art CXR analysis tools and multimodal large language models into a unified framework. MedRAX dynamically leverages these models to address complex medical queries without requiring additional training.</p>
                <p>To rigorously evaluate its capabilities, we introduce ChestAgentBench, a comprehensive benchmark containing 2,500 complex medical queries across 7 diverse categories. Our experiments demonstrate that MedRAX achieves state-of-the-art performance compared to both open-source and proprietary models, representing a significant step toward the practical deployment of automated CXR interpretation systems.</p>
            </div>
        </div>

        <div class="section" , id="KeyContributions">
            <h2>Key Contributions</h2>
            <div class="subsection">
                <ul style="list-style-type: disc; padding-left: 0; ">
                    <li style="line-height: 1.6; margin-bottom: 10px;">MedRAX, a specialized AI agent framework that seamlessly integrates multiple CXR analysis tools without additional training, dynamically orchestrating specialized components for complex medical queries.</li>
                    <li style="line-height: 1.6; margin-bottom: 10px;">ChestAgentBench, a comprehensive evaluation framework with 2,500 complex medical queries across 7 categories, built from 675 expert-curated clinical cases to assess multi-step reasoning in CXR interpretation.</li>
                    <li style="line-height: 1.6; margin-bottom: 10px;">Experiments show that MedRAX outperforms both general-purpose and biomedical specialist models, demonstrating substantial improvements in complex reasoning tasks while maintaining transparent workflows.</li>
                    <li style="line-height: 1.6; margin-bottom: 10px;">Development of a user-friendly interface, enabling flexible deployment options from local to cloud-based solutions that address healthcare privacy requirements.</li>
                </ul>
            </div>
        </div>

        <div class="section" , id="MedRAXFramework">
            <h2>MedRAX Framework</h2>
            <div class="subsection">
                <p>We present MedRAX, an open-source agent-based framework that can dynamically reason, plan, and execute multi-step CXR workflows. MedRAX integrates multimodal reasoning abilities with structured tool-based decision-making, allowing real-time CXR interpretation without unnecessary computational overhead. Our framework integrates heterogeneous machine learning models—from lightweight classifiers to large LMMs—specialized for diverse downstream tasks, allowing it to decompose and solve complex medical queries by reasoning across multiple analytical skills. </p>
                <figure>
                    <img src="static\images\fig1.png" , class="image" alt="">
                    <figcaption>Figure 1</figcaption>
                </figure>
                <figure>
                    <img src="static\images\fig2.png" class="image" alt="">
                    <!-- <figcaption>Figure 2</figcaption> -->
                </figure>
            </div>
        </div>


        <div class="section" , id="ChestAgentBench">
            <h2>ChestAgentBench</h2>
            <div class="subsection">
                <p>ChestAgentBench is a medical VQA benchmark that offers several distinctive advantages:</p>
                <ul style="list-style-type: disc; padding-left: 30px; ">
                    <li style="line-height: 1.6; margin-bottom: 10px;">It represents one of the largest medical VQA benchmarks, with 2,500 questions derived from 675 expert-validated clinical cases, each with comprehensive radiological findings, detailed discussions, and multi-modal imaging data.</li>
                    <li style="line-height: 1.6; margin-bottom: 10px;">The benchmark combines complex multi-step reasoning assessment with a structured six-choice format, enabling both rigorous evaluation of advanced reasoning capabilities and straightforward, reproducible evaluation.</li>
                    <li style="line-height: 1.6; margin-bottom: 10px;">The benchmark features diverse questions across seven core competencies in CXR interpretation, requiring integration of multiple visual findings and reasoning to mirror the complexity of real-world clinical decision-making.</li>
                </ul>
                <p></p>
                <p>We established seven core competencies alongside reasoning that are essential for CXR interpretation:</p>
                <ul style="list-style-type: disc; padding-left: 30px; ">
                    <li style="line-height: 1.6; margin-bottom: 10px;"><strong>Detection:</strong> Identifying specific findings. (e.g., "Is there a nodule present in the right upper lobe?")</li>
                    <li style="line-height: 1.6; margin-bottom: 10px;"><strong>Classification:</strong> Classifying specific findings. (e.g., "Is this mass benign or malignant in appearance?")</li>
                    <li style="line-height: 1.6; margin-bottom: 10px;"><strong>Localization:</strong> Precise positioning of findings. (e.g., "In which bronchopulmonary segment is the mass located?")</li>
                    <li style="line-height: 1.6; margin-bottom: 10px;"><strong>Comparison:</strong> Analyzing relative sizes and positions. (e.g., "How has the pleural effusion volume changed compared to prior imaging?")</li>
                    <li style="line-height: 1.6; margin-bottom: 10px;"><strong>Relationship:</strong> Understanding relationship of findings. (e.g., "Does the mediastinal lymphadenopathy correlate with the lung mass?")</li>
                    <li style="line-height: 1.6; margin-bottom: 10px;"><strong>Diagnosis:</strong> Interpreting findings for clinical decisions. (e.g., "Given the CXR, what is the likely diagnosis?")</li>
                    <li style="line-height: 1.6; margin-bottom: 10px;"><strong>Characterization:</strong> Describing specific finding attributes. (e.g., "What are the margins of the nodule - smooth, spiculated, or irregular?")</li>
                    <li style="line-height: 1.6; margin-bottom: 10px;"><strong>Reasoning:</strong> Explaining medical rationale and thought. (e.g., "Why do these findings suggest infectious rather than malignant etiology?")</li>
                </ul>
                <figure>
                    <img src="static\images\fig3.png" , class="image" alt="">
                    <!-- <figcaption>Figure 3</figcaption> -->
                </figure>
            </div>
        </div>

        <div class="section" , id="Experiments">
            <h2>Experiments</h2>
            <div class="subsection">
                <p>We evaluate MedRAX against four models: LLaVA-Med, a finetuned LLaVA-13B model for biomedical visual question answering (Li et al. 2024), CheXagent, a Vicuna-13B VLM trained for CXR interpretation (CheXagent), along with GPT-4o and Llama-3.2-90B Vision as popular closed and open-source multimodal LLMs respectively.</p>
                <p>We evaluate models on two complementary benchmarks:</p>
                <ul style="list-style-type: none; padding-left: 15px; ">
                    <li style="line-height: 1.6; margin-bottom: 10px;">(1) ChestAgentBench, our proposed benchmark, which assesses comprehensive CXR reasoning through 2,500 six-choice questions across seven categories: detection, classification, localization, comparison, relationship, characterization, and diagnosis. Model performance is measured by accuracy across all questions.</li>
                    <li style="line-height: 1.6; margin-bottom: 10px;">(2) CheXbench, a popular benchmark that evaluates seven clinically-relevant CXR interpretation tasks. We specifically focus on the visual question answering (238 questions from Rad-Restruct and SLAKE datasets) and fine-grained image-text reasoning (380 questions from OpenI dataset) subsets, as they most closely mirror complex clinical workflows that require precise differentiation between similar findings.</li>
                </ul>
                <figure>
                    <img src="static\images\fig4.png" , class="image" alt="">
                    <!-- <figcaption>Figure 4</figcaption> -->
                </figure>
            </div>
        </div>

        <div class="section" , id="CaseStudy">
            <h2>Case Study</h2>
            <p>We present two representative cases that compare MedRAX to GPT-4o.</p>
            <div class="subsection", id="6a">
                <h2>Medical Device Identification (Eurorad Case 17576)</h2>
                <p>This question asks the model to determine the type of tube present in the CXR. GPT-4o incorrectly suggests an endotracheal tube based on the central positiong of the tube alone. MedRAX, integrated findings from multiple tools like report generation and visual QA, and correctly identifies a chest tube despite one tool (LLaVA-Med) suggesting otherwise. This demonstrates MedRAX's ability to resolve conflicting tool outputs through systematic reasoning.</p>
            </div>
            <div class="subsection", id="6b">
                <h2>Multi-step Disease Diagnosis (Eurorad Case 16703)</h2>
                <p>This questions asks about diagnosing the predominant disease and comparing its severity across lungs. GPT-4o misinterprets the CXR as showing pneumonia with right lung predominance. MedRAX, through sequential tool application of report generation for disease identification and segmentation for lung opacity analysis, correctly determines left pneumothorax as the main finding. This demonstrates MedRAX's ability to break down complex queries into targeted analytical steps.</p>
                <figure>
                    <img src="static\images\fig5.png" , class="image" alt="">
                    <!-- <figcaption>Figure 5</figcaption> -->
                </figure>
            </div>
        </div>

        <div class="section" , id="Conclusion">
            <h2>Conclusion</h2>
            <div class="subsection">
                <p>MedRAX establishes a new benchmark in AI-driven CXR interpretation by integrating structured tool orchestration with large-scale reasoning. Our evaluation on ChestAgentBench demonstrates its superiority over both general-purpose and domain-specific models, reinforcing the advantages of explicit stepwise reasoning in medical AI. These findings highlight the potential of combining foundation models with specialized tools, a principle that could be applied to broader domains in healthcare and beyond. Future work should focus on optimizing tool selection, uncertainty-aware reasoning, and expanding MedRAX's capabilities to multimodal medical imaging for greater clinical impact.</p>
            </div>
        </div>

        <div class="section" , id="Reference">
            <h2>Reference</h2>
            <div class="subsection">
                <p>[TBD]</p>
            </div>
        </div>

        <div id="BibTeX">
            <h2>BibTeX</h2>
<code id="myCode">@misc{fallahpour2025medraxmedicalreasoningagent,
    title={MedRAX: Medical Reasoning Agent for Chest X-ray}, 
    author={Adibvafa Fallahpour and Jun Ma and Alif Munim and Hongwei Lyu and Bo Wang},
    year={2025},
    eprint={2502.02673},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2502.02673}, 
}</code>
            <button id="copyButton">Copy</button>
        </div>
    </div>

</body>

</html>